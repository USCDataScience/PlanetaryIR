{
  "name": "Planetary Information Retrieval",
  "tagline": "",
  "body": "# PlanetaryIR\r\nAutomatic Extraction of Planetary Science Literature Authors, Targets and Compositions using Deep Dive and Tika\r\n\r\n\r\nBACKGROUND: \r\n\r\nThere are continuous and growing contributions made to the body of knowledge representing discoveries and conclusions about various targets on the surface of Mars. This body of knowledge is represented by text, whether on the web in the form of HTML documents or as concise documents uploaded to the web. Planetary scientists and mission planners use search tools to update themselves about these new discoveries made about surface targets on Mars. However, these tools that mine the corpus of information are not currently built to meet the knowledge needs of scientists. Hence arises a need to build an information extraction system that can parse text and convert them into data usable by the mission planners and scientists. This data is the input that goes into building an encyclopedia for Mars targets. \r\n\t\r\nThis project is aimed at extracting the authors, targets and compositions of each contribution to the encyclopedia, which are essentially articles extracted from the web and/or other sources of data related to target discoveries on Mars. We will use Deep Dive integrated with Tika to extract this information and Deep Dive for reasoning based on the extracted data.\r\n\r\nAPPROACH:\r\n\r\nPlanetary science findings and conclusions exist in a vast body of knowledge in a number of formats. Apache Tika provides a generic API to detect and extract data from these multiple file formats. \r\n\r\nAs a first step in this project, we will use Tika to parse the target discovery data which are either in the PDF or HTML format. In the next stage, we will use Deep Dive to convert the parsed text from the literature into structured data which can be organized as discoveries and mentions of authors who have made contributions to literature related to such discoveries. This creates an automatic mapping from scholarly articles about target discoveries and conclusions on Mars to authors who have contributed to it. \r\n\r\nWe will thereby use Deep Dive hand in hand with Tika to parse text and create relationships between text and their corresponding authors.\r\n\r\n\r\nTIMELINE:\r\n\r\nAvailability: 2hrs/week\r\n\r\nWeek\r\n1 & 2\r\nUnderstand current work, completed components and do required project setup\r\n\r\n3-6\r\nExtraction using Tika\r\nDemo Tika extraction after 6th week\r\n\r\n7-10\r\nConnect using Deep Dive\r\n\r\n11-12\r\nConclusion & Demo\r\nFinal Deliverable\r\n\r\nDELIVERABLES:\r\n\r\n1)\tBy the start of 7th week, working model for content extraction of the literature and text\r\n2)\tFor the final demo, connected core components with a system that uses Deep Dive to map authors to planetary science literature from the parsed text.\r\n\r\nSTEPS TO INSTALL:\r\n\r\n1) Install DeepDive and postgreSQL according to the instructions at http://deepdive.stanford.edu/installation\r\n\r\nYou should now be able to see Deep Dive and its dependencies installed at /local under your home directory\r\n\r\n2) Do \"brew install graphviz\" and remove deepdive installation's bundled one with rm -rf $(path to bundled graphviz in DeepDive)\r\n\r\nThe path to bundled graphviz is usually /local/lib/bundled\r\n\r\n3) Clone repo into your machine and note the path where you have cloned the project into \r\n\r\n4) Launch terminal and switch to app/planetaryIR directory under the cloned repo\r\n\r\n5) Run input/init.sh script file to install bazaar dependency which is the CoreNLP parser\r\n\r\n6) Run deepdive compare\r\n\r\n7) Run \"deepdive do articles\" - This will load the articles from the json file into the database\r\n\r\n8) Run \"deepdive query '?- articles(\"1438\", content).' format=csv | grep -v '^$' | tail -n +16 | head\" - This is to check whether the article text got loaded successfully\r\n\r\n9) Run \"deepdive do sentences\" - This will run the NLP parser on the articles table and output into the setnences table with their NER tags\r\n\r\n10) Run \"deepdive query '?- sentences(\"1438\", _, _, tokens, _, _, ner_tags, _, _, _).' format=csv | grep PERSON | tail\" to see these NER tags\r\n\r\n11) Run \"deepdive do person_mention\" or \"deepdive redo person_mention\" whichever the case maybe to perform author extractions\r\n\r\n12) Go to postgreSQL terminal and run \"COPY person_mention TO 'path to where you want this file/authors.csv' DELIMITER ',' CSV HEADER;\"\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}